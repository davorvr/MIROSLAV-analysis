{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1e6d5b",
   "metadata": {},
   "source": [
    "# Prepare-a-SLAV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5b129",
   "metadata": {},
   "source": [
    "Prepare-a-SLAV utilises the mirofile library to load raw MIROSLAV data into a pandas data frame. Appropriate animal IDs are applied, and the data is downsampled from the default MIROSLAV sampling rate (10 sensor readings/binary values per second) to an arbitrary, user-defined time interval (bin). Prepare-a-SLAV's configuration is performed through the Prepare-a-SLAV TOML configuration file where you can find more information about its parameters.\n",
    "\n",
    "If you are running Prepare-a-SLAV via Google Colab, Prepare-a-SLAV will autodetect and set up the Colab environment in the following cell, and pull example data and the TOML configuration file from the [MIROSLAV toolkit GitHub repository](https://github.com/davorvr/MIROSLAV-analysis).\n",
    "\n",
    "If you want to run Prepare-a-SLAV in Google Colab *and* with your own data, you can upload your configuration and data files using the File Browser in the sidebar on the left after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103503fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False\n",
    "    pass\n",
    "else:\n",
    "    import sys\n",
    "    from IPython.display import clear_output \n",
    "    clear_output()\n",
    "    %pip install --ignore-requires-python mirofile\n",
    "    %pip install pandas --upgrade\n",
    "    %pip install fastparquet\n",
    "    !wget https://raw.githubusercontent.com/davorvr/MIROSLAV-analysis/main/1_Prepare-a-SLAV_config.toml\n",
    "    !mkdir 0_raw_logs\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_M.2022-05-16T01-33-25-478055.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-16T01-33-25-478055.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_M.2022-05-25T14-03-17-240158.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-25T14-03-17-240158.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_R.2022-05-06T19-19-57-669185.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-06T19-19-57-669185.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_R.2022-05-16T01-33-22-935712.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-16T01-33-22-935712.gz\n",
    "    !wget -O 0_raw_logs/mph-pir-rack_R.2022-05-25T07-57-01-575482.gz https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-25T07-57-01-575482.gz\n",
    "    clear_output()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f1592",
   "metadata": {},
   "source": [
    "The environment has been set up. If you wish, you can load your own data using the sidebar now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d283b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1580e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if IN_COLAB and sys.hexversion < 0x030b0000:\n",
    "    OLD_TOML = True\n",
    "    import toml\n",
    "else:\n",
    "    OLD_TOML = False\n",
    "    import tomllib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from mirofile import mirofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737694c",
   "metadata": {},
   "source": [
    "Define helper functions for managing imported column mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_mcp_colmap(colmap: dict):\n",
    "    unpacked_colmap = {}\n",
    "    # first do PH7..0\n",
    "    for i in range(7,-1,-1):\n",
    "        unpacked_colmap.update({\"PH\"+str(i):\"H_\"+colmap[\"PHL\"+str(i)]})\n",
    "    # then do PL0..7\n",
    "    for i in range(0,8):\n",
    "        unpacked_colmap.update({\"PL\"+str(i):\"L_\"+colmap[\"PHL\"+str(i)]})\n",
    "    return unpacked_colmap\n",
    "\n",
    "def unpack_full_colmap(colmap: dict[dict]):\n",
    "    unpacked_colmap = {}\n",
    "    input_boards = list(colmap.keys())\n",
    "    input_boards.remove(\"top_board\")\n",
    "    input_boards.sort()\n",
    "    input_boards = [\"top_board\"] + input_boards\n",
    "    for mcp_i, colmap_name in enumerate(input_boards):\n",
    "        mcp_colmap = colmap[colmap_name]\n",
    "        mcp_colmap_unpacked = _unpack_mcp_colmap(mcp_colmap)\n",
    "        # new_mcp_colmap = {}\n",
    "        for k, v in mcp_colmap_unpacked.items():\n",
    "            #new_mcp_colmap.update({f\"MCP{mcp_i+1}_\"+k : v})\n",
    "            unpacked_colmap.update({f\"MCP{mcp_i+1}_\"+k : v})\n",
    "        #unpacked_colmap.append(new_mcp_colmap.copy())\n",
    "    return unpacked_colmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ff5de",
   "metadata": {},
   "source": [
    "Set the current working directory to the location of this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881a9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = Path(os.path.dirname(os.path.realpath('__file__'))).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a6bb2",
   "metadata": {},
   "source": [
    "Load the TOML config file and extract all user-defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OLD_TOML:\n",
    "    with open(wd / \"1_Prepare-a-SLAV_config.toml\", \"r\") as cfg_file:\n",
    "        config = toml.load(cfg_file)\n",
    "else:\n",
    "    with open(wd / \"1_Prepare-a-SLAV_config.toml\", \"rb\") as cfg_file:\n",
    "        config = tomllib.load(cfg_file)\n",
    "\n",
    "try:\n",
    "    experiment = config[\"id_variables\"][\"experiment\"]\n",
    "    set_dtypes = config[\"processing_params\"][\"set_dtypes\"]\n",
    "    resample = config[\"processing_params\"][\"resample\"]\n",
    "    resample_bin = config[\"processing_params\"][\"resample_bin\"]\n",
    "    toml_colnames = config[experiment]\n",
    "except KeyError as exc:\n",
    "    raise KeyError(\"Config file is improperly formatted!\") from exc\n",
    "\n",
    "colmaps = {}\n",
    "for k, v in toml_colnames.items():\n",
    "    try:\n",
    "        do_process = v.pop(\"process\")\n",
    "    except KeyError as exc:\n",
    "        raise KeyError(\"Config file is improperly formatted!\") from exc\n",
    "    if not do_process:\n",
    "        continue\n",
    "    try:\n",
    "        v = unpack_full_colmap(v)\n",
    "    except Exception as exc:\n",
    "        raise KeyError(\"Couldn't process cage mappings from config file!\") from exc\n",
    "    colmaps.update({k: v})\n",
    "\n",
    "log_path = Path.cwd() / \"0_raw_logs\"\n",
    "output_path = Path.cwd() / \"1_outputs_prepared\"\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4590025",
   "metadata": {},
   "source": [
    "The number of rows read at once. Reduce if you run into memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e69a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f5085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing device rack_M. \n",
      "(rack_M) Processing chunk 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:07<00:00, 130730.55it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 199950.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-07 23:06:00\n",
      "(rack_M) Processing chunk 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999615/999615 [00:06<00:00, 147381.35it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 151391.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-09 02:53:00\n",
      "(rack_M) Processing chunk 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 155170.97it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 146151.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-10 06:40:00\n",
      "(rack_M) Processing chunk 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 147166.06it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 120111.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-11 10:27:00\n",
      "(rack_M) Processing chunk 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:06<00:00, 158758.20it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 100011.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-12 14:14:00\n",
      "(rack_M) Processing chunk 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 158953.05it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 149957.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-13 18:01:00\n",
      "(rack_M) Processing chunk 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 150724.34it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 108872.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-14 21:48:00\n",
      "(rack_M) Processing chunk 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:07<00:00, 136944.73it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 120111.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-16 01:35:00\n",
      "(rack_M) Processing chunk 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 151292.37it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 200094.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-17 05:22:00\n",
      "(rack_M) Processing chunk 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 144384.19it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 120025.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-18 09:09:00\n",
      "(rack_M) Processing chunk 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:08<00:00, 121456.86it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 117224.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-19 12:56:00\n",
      "(rack_M) Processing chunk 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:07<00:00, 137905.88it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 119951.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-20 16:43:00\n",
      "(rack_M) Processing chunk 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999581/999581 [00:06<00:00, 153022.04it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 200062.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-21 20:30:00\n",
      "(rack_M) Processing chunk 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 147260.75it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 120111.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-23 00:17:00\n",
      "(rack_M) Processing chunk 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:07<00:00, 140558.22it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 119980.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-24 04:04:00\n",
      "(rack_M) Processing chunk 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:06<00:00, 150468.68it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 75003.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-25 14:08:00\n",
      "(rack_M) Processing chunk 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999958/999958 [00:07<00:00, 125487.10it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 298668.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-26 17:55:00\n",
      "(rack_M) Processing chunk 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:06<00:00, 149806.70it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 74985.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-27 21:42:00\n",
      "(rack_M) Processing chunk 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999590/999590 [00:06<00:00, 148348.15it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 150082.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-29 01:29:00\n",
      "(rack_M) Processing chunk 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999591/999591 [00:06<00:00, 151916.71it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 149121.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-30 05:16:00\n",
      "(rack_M) Processing chunk 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999592/999592 [00:06<00:00, 153768.01it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 199903.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-31 09:03:00\n",
      "(rack_M) Processing chunk 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350064/999592 [00:02<00:04, 152025.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_M) Chunk processed. Last timestamp: 2022-05-31 18:48:00\n",
      "(rack_M) Processing chunk 23...\n",
      "mph-pir-rack_M-dtyped-resampled-1minute.parquet:  0:05:16.964134\n",
      "Processing device rack_R. \n",
      "(rack_R) Processing chunk 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:05<00:00, 172155.62it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 113089.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-07 23:06:00\n",
      "(rack_R) Processing chunk 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999618/999618 [00:06<00:00, 150666.85it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 150055.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-09 02:53:00\n",
      "(rack_R) Processing chunk 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:06<00:00, 154786.67it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 145990.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-10 06:40:00\n",
      "(rack_R) Processing chunk 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:06<00:00, 162791.69it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 148208.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-11 10:27:00\n",
      "(rack_R) Processing chunk 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:05<00:00, 186177.52it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 200141.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-12 14:14:00\n",
      "(rack_R) Processing chunk 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:06<00:00, 166405.77it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 149716.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-13 18:01:00\n",
      "(rack_R) Processing chunk 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:06<00:00, 156971.00it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 150037.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-14 21:48:00\n",
      "(rack_R) Processing chunk 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:05<00:00, 166684.48it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 132277.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-16 01:35:00\n",
      "(rack_R) Processing chunk 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:06<00:00, 163526.92it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 205100.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-17 05:22:00\n",
      "(rack_R) Processing chunk 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:07<00:00, 135626.04it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 149823.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-18 09:09:00\n",
      "(rack_R) Processing chunk 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:06<00:00, 146232.49it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 149680.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-19 12:56:00\n",
      "(rack_R) Processing chunk 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999595/999595 [00:08<00:00, 116446.99it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 151336.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-20 16:43:00\n",
      "(rack_R) Processing chunk 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999584/999584 [00:05<00:00, 179896.94it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 199554.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-21 20:30:00\n",
      "(rack_R) Processing chunk 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:04<00:00, 223844.84it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 198374.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-23 00:17:00\n",
      "(rack_R) Processing chunk 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:04<00:00, 209779.45it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 292150.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-24 04:04:00\n",
      "(rack_R) Processing chunk 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:04<00:00, 213886.99it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 199554.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-25 08:01:00\n",
      "(rack_R) Processing chunk 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999468/999468 [00:04<00:00, 211087.09it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 311073.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-26 11:48:00\n",
      "(rack_R) Processing chunk 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999593/999593 [00:04<00:00, 214173.09it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 199538.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-27 15:35:00\n",
      "(rack_R) Processing chunk 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:04<00:00, 216903.12it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 150172.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-28 19:22:00\n",
      "(rack_R) Processing chunk 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999594/999594 [00:04<00:00, 211655.60it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 171335.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-29 23:09:00\n",
      "(rack_R) Processing chunk 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999595/999595 [00:04<00:00, 212544.86it/s]\n",
      "100%|██████████| 600/600 [00:00<00:00, 190462.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-31 02:56:00\n",
      "(rack_R) Processing chunk 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 570267/999595 [00:02<00:01, 216555.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rack_R) Chunk processed. Last timestamp: 2022-05-31 18:48:00\n",
      "(rack_R) Processing chunk 23...\n",
      "mph-pir-rack_R-dtyped-resampled-1minute.parquet:  0:04:00.568651\n"
     ]
    }
   ],
   "source": [
    "ts_column = \"ts_recv\"\n",
    "ts_delta = True\n",
    "ts_index_map = {\"ts_recv\": 0,\n",
    "                \"ts_sent\": 1}\n",
    "ts_index = ts_index_map[ts_column]\n",
    "resample_bin_td = pd.to_timedelta(resample_bin)\n",
    "\n",
    "for device, colmap in colmaps.items():\n",
    "    print(f\"Processing device {device}. \")\n",
    "    mname = \"-\".join([experiment, \"pir\", device])\n",
    "\n",
    "    mfile = mirofile.open_experiment(experiment, device, compression=\"gz\", path=log_path)\n",
    "    suffix = \"\"\n",
    "    if set_dtypes:\n",
    "        suffix += \"-dtyped\"\n",
    "    if resample:\n",
    "        suffix += f\"-resampled-{resample_bin}\".replace(\" \", \"\")\n",
    "    pqfile = Path(output_path, mname+suffix+\".parquet\")\n",
    "    \n",
    "    if pqfile.exists():\n",
    "        raise FileExistsError(\"Database file already exists, not overwriting!\")\n",
    "        #pass\n",
    "\n",
    "    # Column names for each of the 16 bits outputted by one MCP are stored\n",
    "    # in order in the mirofile.mcp_colnames list (for more explanation, see\n",
    "    # mirofile_columns.txt). But, since we usually have more than one MCP,\n",
    "    # we need to prepend the column names with \"MCPn_\". This line does this:\n",
    "    data_columns = [ f\"MCP{n_mcp}_\"+colname for n_mcp in range(1, mfile.n_mcps+1) for colname in mirofile.mcp_columns ]\n",
    "    all_columns = mirofile.timestamp_columns+data_columns\n",
    "    all_column_dtypes = [*[\"datetime64[ns]\"]*2, *[\"int\"]*(len(all_columns)-2)]\n",
    "\n",
    "    start = datetime.now()\n",
    "    file_chunk_carryover = None\n",
    "    #file_chunk = mfile.readlists(size=chunk_size, progress_bar=True)\n",
    "    sampling_period = None\n",
    "    n_chunk = 0\n",
    "    while True:\n",
    "        n_chunk += 1\n",
    "        print(f\"({device}) Processing chunk {n_chunk}...\")\n",
    "        if isinstance(file_chunk_carryover, pd.DataFrame):\n",
    "            #file_chunk = [file_chunk_carryover]\n",
    "            file_chunk = mfile.readlists(size=chunk_size-len(file_chunk_carryover), progress_bar=True)\n",
    "        else:\n",
    "            file_chunk = mfile.readlists(size=chunk_size, progress_bar=True)\n",
    "        if not file_chunk:\n",
    "            break\n",
    "        file_chunk = pd.DataFrame.from_records(file_chunk, columns=all_columns)\n",
    "        if set_dtypes:\n",
    "            file_chunk = file_chunk.astype(dict(zip(all_columns, all_column_dtypes)))\n",
    "            if isinstance(file_chunk_carryover, pd.DataFrame) and not file_chunk_carryover.empty:\n",
    "                file_chunk = pd.concat([file_chunk_carryover, file_chunk], ignore_index=True)\n",
    "                file_chunk_carryover = None\n",
    "            if not sampling_period:\n",
    "                sampling_period = file_chunk[ts_column].diff().mode().iloc[0]\n",
    "                supp_len = ceil(resample_bin_td / sampling_period)\n",
    "            if resample:\n",
    "                file_chunk_supplement = []\n",
    "                bin_end = file_chunk[ts_column].iloc[-1].ceil(freq=resample_bin_td)\n",
    "                file_chunk_supplement = mfile.readlists(supp_len, progress_bar=True)\n",
    "                if file_chunk_supplement:\n",
    "                    #DEBUG: if pd.to_datetime(last_line[ts_index]) > pd.Timestamp('2022-05-17 05:59:59.000000'):\n",
    "                        #DEBUG: print(\"break here\")\n",
    "                    chunk_end = pd.to_datetime(file_chunk_supplement[-1][ts_index])\n",
    "                    while chunk_end < bin_end:\n",
    "                        file_chunk_supplement.append(mfile.readlists(supp_len, progress_bar=True))\n",
    "                        chunk_end = pd.to_datetime(file_chunk_supplement[-1][ts_index])\n",
    "                        #last_line = mfile.readlist()\n",
    "                \n",
    "                    file_chunk_supplement = pd.DataFrame.from_records(file_chunk_supplement, columns=all_columns)\n",
    "                    file_chunk_supplement = file_chunk_supplement.astype(dict(zip(all_columns, all_column_dtypes)))\n",
    "                    file_chunk = pd.concat([file_chunk, file_chunk_supplement.loc[file_chunk_supplement[ts_column] < bin_end].copy()], ignore_index=True)\n",
    "                    file_chunk_carryover = file_chunk_supplement.loc[file_chunk_supplement[ts_column] >= bin_end].copy()\n",
    "                    file_chunk_supplement = None\n",
    "                else:\n",
    "                    file_chunk_carryover = None\n",
    "\n",
    "        if colmap:\n",
    "            file_chunk = file_chunk.rename(colmap, axis=\"columns\")\n",
    "            file_chunk = file_chunk.loc[:,~file_chunk.columns.str.startswith(\"H_na\")]\n",
    "            file_chunk = file_chunk.loc[:,~file_chunk.columns.str.startswith(\"L_na\")]\n",
    "            file_chunk = file_chunk.copy()\n",
    "        if ts_delta:\n",
    "            if ts_column == \"ts_recv\":\n",
    "                secondary_ts = \"ts_sent\"\n",
    "            elif ts_column == \"ts_sent\":\n",
    "                secondary_ts = \"ts_recv\"\n",
    "            else:\n",
    "                raise ValueError(\"ts_column must be either 'ts_recv' or 'ts_sent'\")\n",
    "            # delta is always ts_recv - ts_sent since ts_recv is always later, but\n",
    "            # we keep the other's name so it's clear which column got replaced with a delta.\n",
    "            file_chunk[secondary_ts+\"_delta\"] = file_chunk[\"ts_recv\"] - file_chunk[\"ts_sent\"]\n",
    "            file_chunk = file_chunk.drop(columns=secondary_ts)\n",
    "        file_chunk = file_chunk.set_index(ts_column)\n",
    "        if resample:\n",
    "            if ts_delta:\n",
    "                file_chunk = file_chunk.resample(resample_bin_td).mean(numeric_only=False)\n",
    "            else:\n",
    "                file_chunk = file_chunk.resample(resample_bin_td).mean(numeric_only=True)\n",
    "            \n",
    "        if not pqfile.exists():\n",
    "            file_chunk.to_parquet(pqfile.absolute(), engine=\"fastparquet\")\n",
    "        else:\n",
    "            file_chunk.to_parquet(pqfile.absolute(), engine=\"fastparquet\", append=True)\n",
    "        #DEBUG: last_chunk = file_chunk.copy()\n",
    "        print(f\"({device}) Chunk processed. Last timestamp: {file_chunk.index[-1]}\")\n",
    "    print(f\"{pqfile.name}: \", datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4638df0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fb8a3",
   "metadata": {},
   "source": [
    "You should be able to obtain the output files from the sidebar now, and proceed to [TidySLAV](https://github.com/davorvr/MIROSLAV-analysis)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
