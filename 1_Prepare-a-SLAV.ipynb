{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1e6d5b",
   "metadata": {},
   "source": [
    "# Prepare-a-SLAV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5b129",
   "metadata": {},
   "source": [
    "Prepare-a-SLAV utilises the mirofile library to load raw MIROSLAV data into a pandas data frame. Appropriate animal IDs are applied, and the data is downsampled from the default MIROSLAV sampling rate (10 sensor readings/binary values per second) to an arbitrary, user-defined time interval (bin). Prepare-a-SLAV's configuration is performed through the Prepare-a-SLAV TOML configuration file where you can find more information about its parameters.\n",
    "\n",
    "If you are running Prepare-a-SLAV via Google Colab, Prepare-a-SLAV will autodetect and set up the Colab environment in the following cell, and pull example data and the TOML configuration file from the [MIROSLAV toolkit GitHub repository](https://github.com/davorvr/MIROSLAV-analysis).\n",
    "\n",
    "If you want to run Prepare-a-SLAV in Google Colab *and* with your own data, you can upload your configuration and data files using the File Browser in the sidebar on the left after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103503fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False\n",
    "    pass\n",
    "else:\n",
    "    %pip install pandas\n",
    "    %pip install mirofile\n",
    "    !wget https://raw.githubusercontent.com/davorvr/MIROSLAV-analysis/main/1_Prepare-a-SLAV_config.toml\n",
    "    !mkdir 0_raw_logs\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-06T19-19-57-669585.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-16T01-33-25-478055.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_M.2022-05-25T14-03-17-240158.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-06T19-19-57-669185.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-16T01-33-22-935712.gz\n",
    "    !wget -P 0_raw_logs https://github.com/davorvr/MIROSLAV-analysis/raw/main/0_raw_logs/mph-pir-rack_R.2022-05-25T07-57-01-575482.gz\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f1592",
   "metadata": {},
   "source": [
    "The environment has been set up. If you wish, you can load your own data using the sidebar now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d283b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1580e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tomllib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from mirofile import mirofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737694c",
   "metadata": {},
   "source": [
    "Define helper functions for managing imported column mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_mcp_colmap(colmap: dict):\n",
    "    unpacked_colmap = {}\n",
    "    # first do PH7..0\n",
    "    for i in range(7,-1,-1):\n",
    "        unpacked_colmap.update({\"PH\"+str(i):\"H_\"+colmap[\"PHL\"+str(i)]})\n",
    "    # then do PL0..7\n",
    "    for i in range(0,8):\n",
    "        unpacked_colmap.update({\"PL\"+str(i):\"L_\"+colmap[\"PHL\"+str(i)]})\n",
    "    return unpacked_colmap\n",
    "\n",
    "def unpack_full_colmap(colmap: dict[dict]):\n",
    "    unpacked_colmap = {}\n",
    "    input_boards = list(colmap.keys())\n",
    "    input_boards.remove(\"top_board\")\n",
    "    input_boards.sort()\n",
    "    input_boards = [\"top_board\"] + input_boards\n",
    "    for mcp_i, colmap_name in enumerate(input_boards):\n",
    "        mcp_colmap = colmap[colmap_name]\n",
    "        mcp_colmap_unpacked = _unpack_mcp_colmap(mcp_colmap)\n",
    "        # new_mcp_colmap = {}\n",
    "        for k, v in mcp_colmap_unpacked.items():\n",
    "            #new_mcp_colmap.update({f\"MCP{mcp_i+1}_\"+k : v})\n",
    "            unpacked_colmap.update({f\"MCP{mcp_i+1}_\"+k : v})\n",
    "        #unpacked_colmap.append(new_mcp_colmap.copy())\n",
    "    return unpacked_colmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ff5de",
   "metadata": {},
   "source": [
    "Set the current working directory to the location of this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = Path(__file__).parent.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a6bb2",
   "metadata": {},
   "source": [
    "Load the TOML config file and extract all user-defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wd / \"1_Prepare-a-SLAV_config.toml\", \"rb\") as cfg_file:\n",
    "    config = tomllib.load(cfg_file)\n",
    "\n",
    "try:\n",
    "    experiment = config[\"id_variables\"][\"experiment\"]\n",
    "    set_dtypes = config[\"processing_params\"][\"set_dtypes\"]\n",
    "    resample = config[\"processing_params\"][\"resample\"]\n",
    "    resample_bin = config[\"processing_params\"][\"resample_bin\"]\n",
    "    toml_colnames = config[experiment]\n",
    "except KeyError as exc:\n",
    "    raise KeyError(\"Config file is improperly formatted!\") from exc\n",
    "\n",
    "colmaps = {}\n",
    "for k, v in toml_colnames.items():\n",
    "    try:\n",
    "        do_process = v.pop(\"process\")\n",
    "    except KeyError as exc:\n",
    "        raise KeyError(\"Config file is improperly formatted!\") from exc\n",
    "    if not do_process:\n",
    "        continue\n",
    "    try:\n",
    "        v = unpack_full_colmap(v)\n",
    "    except Exception as exc:\n",
    "        raise KeyError(\"Couldn't process cage mappings from config file!\") from exc\n",
    "    colmaps.update({k: v})\n",
    "\n",
    "log_path = Path.cwd() / \"0_raw_logs\"\n",
    "output_path = Path.cwd() / \"1_outputs_prepared\"\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4590025",
   "metadata": {},
   "source": [
    "The number of rows read at once. Reduce if you run into memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e69a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_column = \"ts_recv\"\n",
    "ts_delta = True\n",
    "ts_index_map = {\"ts_recv\": 0,\n",
    "                \"ts_sent\": 1}\n",
    "ts_index = ts_index_map[ts_column]\n",
    "resample_bin_td = pd.to_timedelta(resample_bin)\n",
    "\n",
    "for device, colmap in colmaps.items():\n",
    "    print(f\"Processing device {device}. \")\n",
    "    mname = \"-\".join([experiment, \"pir\", device])\n",
    "\n",
    "    mfile = mirofile.open_experiment(experiment, device, compression=\"gz\", path=log_path)\n",
    "    suffix = \"\"\n",
    "    if set_dtypes:\n",
    "        suffix += \"-dtyped\"\n",
    "    if resample:\n",
    "        suffix += f\"-resampled-{resample_bin}\".replace(\" \", \"\")\n",
    "    pqfile = Path(output_path, mname+suffix+\".parquet\")\n",
    "    \n",
    "    if pqfile.exists():\n",
    "        raise FileExistsError(\"Database file already exists, not overwriting!\")\n",
    "        #pass\n",
    "\n",
    "    # Column names for each of the 16 bits outputted by one MCP are stored\n",
    "    # in order in the mirofile.mcp_colnames list (for more explanation, see\n",
    "    # mirofile_columns.txt). But, since we usually have more than one MCP,\n",
    "    # we need to prepend the column names with \"MCPn_\". This line does this:\n",
    "    data_columns = [ f\"MCP{n_mcp}_\"+colname for n_mcp in range(1, mfile.n_mcps+1) for colname in mirofile.mcp_columns ]\n",
    "    all_columns = mirofile.timestamp_columns+data_columns\n",
    "    all_column_dtypes = [*[\"datetime64[ns]\"]*2, *[\"int\"]*(len(all_columns)-2)]\n",
    "\n",
    "    start = datetime.now()\n",
    "    file_chunk_carryover = None\n",
    "    #file_chunk = mfile.readlists(size=chunk_size, progress_bar=True)\n",
    "    sampling_period = None\n",
    "    n_chunk = 0\n",
    "    while True:\n",
    "        n_chunk += 1\n",
    "        print(f\"({device}) Processing chunk {n_chunk}...\")\n",
    "        if isinstance(file_chunk_carryover, pd.DataFrame):\n",
    "            #file_chunk = [file_chunk_carryover]\n",
    "            file_chunk = mfile.readlists(size=chunk_size-len(file_chunk_carryover), progress_bar=True)\n",
    "        else:\n",
    "            file_chunk = mfile.readlists(size=chunk_size, progress_bar=True)\n",
    "        if not file_chunk:\n",
    "            break\n",
    "        file_chunk = pd.DataFrame.from_records(file_chunk, columns=all_columns)\n",
    "        if set_dtypes:\n",
    "            file_chunk = file_chunk.astype(dict(zip(all_columns, all_column_dtypes)))\n",
    "            if isinstance(file_chunk_carryover, pd.DataFrame) and not file_chunk_carryover.empty:\n",
    "                file_chunk = pd.concat([file_chunk_carryover, file_chunk], ignore_index=True)\n",
    "                file_chunk_carryover = None\n",
    "            if not sampling_period:\n",
    "                sampling_period = file_chunk[ts_column].diff().mode().iloc[0]\n",
    "                supp_len = ceil(resample_bin_td / sampling_period)\n",
    "            if resample:\n",
    "                file_chunk_supplement = []\n",
    "                bin_end = file_chunk[ts_column].iloc[-1].ceil(freq=resample_bin_td)\n",
    "                file_chunk_supplement = mfile.readlists(supp_len, progress_bar=True)\n",
    "                if file_chunk_supplement:\n",
    "                    #DEBUG: if pd.to_datetime(last_line[ts_index]) > pd.Timestamp('2022-05-17 05:59:59.000000'):\n",
    "                        #DEBUG: print(\"break here\")\n",
    "                    chunk_end = pd.to_datetime(file_chunk_supplement[-1][ts_index])\n",
    "                    while chunk_end < bin_end:\n",
    "                        file_chunk_supplement.append(mfile.readlists(supp_len, progress_bar=True))\n",
    "                        chunk_end = pd.to_datetime(file_chunk_supplement[-1][ts_index])\n",
    "                        #last_line = mfile.readlist()\n",
    "                \n",
    "                    file_chunk_supplement = pd.DataFrame.from_records(file_chunk_supplement, columns=all_columns)\n",
    "                    file_chunk_supplement = file_chunk_supplement.astype(dict(zip(all_columns, all_column_dtypes)))\n",
    "                    file_chunk = pd.concat([file_chunk, file_chunk_supplement.loc[file_chunk_supplement[ts_column] < bin_end].copy()], ignore_index=True)\n",
    "                    file_chunk_carryover = file_chunk_supplement.loc[file_chunk_supplement[ts_column] >= bin_end].copy()\n",
    "                    file_chunk_supplement = None\n",
    "                else:\n",
    "                    file_chunk_carryover = None\n",
    "\n",
    "        if colmap:\n",
    "            file_chunk = file_chunk.rename(colmap, axis=\"columns\")\n",
    "            file_chunk = file_chunk.loc[:,~file_chunk.columns.str.startswith(\"H_na\")]\n",
    "            file_chunk = file_chunk.loc[:,~file_chunk.columns.str.startswith(\"L_na\")]\n",
    "        if ts_delta:\n",
    "            if ts_column == \"ts_recv\":\n",
    "                secondary_ts = \"ts_sent\"\n",
    "            elif ts_column == \"ts_sent\":\n",
    "                secondary_ts = \"ts_recv\"\n",
    "            else:\n",
    "                raise ValueError(\"ts_column must be either 'ts_recv' or 'ts_sent'\")\n",
    "            # delta is always ts_recv - ts_sent since ts_recv is always later, but\n",
    "            # we keep the other's name so it's clear which column got replaced with a delta.\n",
    "            file_chunk[secondary_ts+\"_delta\"] = file_chunk[\"ts_recv\"] - file_chunk[\"ts_sent\"]\n",
    "            file_chunk = file_chunk.drop(columns=secondary_ts)\n",
    "        file_chunk = file_chunk.set_index(ts_column)\n",
    "        if resample:\n",
    "            if ts_delta:\n",
    "                file_chunk = file_chunk.resample(resample_bin_td).mean(numeric_only=False)\n",
    "            else:\n",
    "                file_chunk = file_chunk.resample(resample_bin_td).mean(numeric_only=True)\n",
    "            \n",
    "        if not pqfile.exists():\n",
    "            file_chunk.to_parquet(pqfile.absolute(), engine=\"fastparquet\")\n",
    "        else:\n",
    "            file_chunk.to_parquet(pqfile.absolute(), engine=\"fastparquet\", append=True)\n",
    "        #DEBUG: last_chunk = file_chunk.copy()\n",
    "        print(f\"({device}) Chunk processed. Last timestamp: {file_chunk.index[-1]}\")\n",
    "    print(f\"{pqfile.name}: \", datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4638df0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fb8a3",
   "metadata": {},
   "source": [
    "You should be able to obtain the output files from the sidebar now, and proceed to [TidySLAV](https://github.com/davorvr/MIROSLAV-analysis)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
